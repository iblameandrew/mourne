"""
Master Planner for Mourne.
Decomposes a creative script into granular visual steps for media generation.
"""
import json
from typing import Optional
from .llm_backend import get_planner_llm, OpenRouterLLM
from .models import MasterPlan, SceneStep, MediaType, VoiceDirection, VoiceType, VoiceGender


PLANNER_SYSTEM_PROMPT = """You are a Master Video Planner for cinematic AI-generated music videos.
You break down creative scripts into granular visual scenes that can be generated by AI.

Your output must be valid JSON that follows the exact schema provided.
Be creative, cinematic, and precise with timing."""


PLANNER_PROMPT_TEMPLATE = """
Given a creative script and audio analysis, break it down into GRANULAR visual scenes.
Each scene should be 2-8 seconds and represent a single visual moment.

**Input:**
- Script/Concept: {script}
- Audio/Lyrics Analysis: {audio_analysis}
- Total Duration: {duration} seconds

**Output Format (JSON):**
{{
    "project_name": "A creative name for this video project",
    "total_duration": {duration},
    "scenes": [
        {{
            "scene_number": 1,
            "description": "Brief narrative description of what happens",
            "time_start": 0.0,
            "time_end": 4.0,
            "suggested_media_type": "image",
            "visual_prompt_draft": "Detailed visual prompt for AI generation - be VERY specific about composition, lighting, style",
            "audio_context": "What's happening in the audio at this moment",
            "mood": "emotional tone (e.g., melancholic, energetic, ethereal)",
            "suggested_transition": "crossfade",
            "voice": {{
                "should_speak": true,
                "voice_type": "narrator",
                "dialogue_text": "The exact words to speak (if speaking)",
                "tone": 0.5,
                "cadence": 0.5,
                "warmth": 0.5,
                "solemnity": 0.5,
                "gender": "androgynous",
                "age_hint": "adult",
                "voice_notes": "Director notes for voice delivery"
            }}
        }},
        ...more scenes to cover the ENTIRE duration...
    ]
}}

**Rules:**
1. Use "video" for dynamic moments (movement, action, camera motion, transitions between states)
2. Use "image" for static/contemplative moments (can apply Ken Burns effect later)
3. Visual prompts should be HIGHLY detailed and cinematic:
   - Describe camera angle (close-up, wide shot, aerial, dutch angle)
   - Describe lighting (golden hour, neon glow, chiaroscuro, soft diffused)
   - Describe style (photorealistic, anime, oil painting, film noir)
   - Describe specific details (textures, colors, atmosphere)
4. Ensure scenes cover the ENTIRE duration with no gaps
5. Match visual mood to audio mood precisely
6. Each scene's time_end should equal the next scene's time_start
7. Transitions: "fade", "crossfade", "cut", "zoom", or "slide"

**Voice Direction Rules:**
8. Not every scene needs voice - silence can be powerful
9. Voice types: "narrator" (omniscient), "character" (entity speaking), "inner_thought" (internal monologue), "none"
10. Calibrate voice parameters (0.0 to 1.0):
    - tone: 0=dark/serious, 1=bright/uplifting
    - cadence: 0=slow/deliberate, 1=fast/energetic
    - warmth: 0=cold/distant, 1=warm/intimate
    - solemnity: 0=casual/playful, 1=solemn/reverent
11. Ensure dialogue_text length fits the scene duration
12. Vary voice presence across scenes for dynamic storytelling

Return ONLY valid JSON. No explanation, no markdown code blocks.
"""


class MasterPlanner:
    """
    Decomposes a creative script into granular visual steps.
    Each step becomes a sub-agent task for media generation.
    """
    
    def __init__(self, llm: Optional[OpenRouterLLM] = None):
        self.llm = llm or get_planner_llm()
    
    async def create_plan(
        self, 
        script: str, 
        audio_analysis: str, 
        duration: float
    ) -> MasterPlan:
        """
        Generate a complete scene-by-scene plan.
        
        Args:
            script: The creative concept or script for the video
            audio_analysis: Transcription or description of the audio track
            duration: Total video duration in seconds
        
        Returns:
            MasterPlan with ordered list of SceneSteps
        """
        
        prompt = PLANNER_PROMPT_TEMPLATE.format(
            script=script,
            audio_analysis=audio_analysis,
            duration=duration
        )
        
        result = await self.llm.generate_json(
            prompt=prompt,
            system=PLANNER_SYSTEM_PROMPT,
            temperature=0.6
        )
        
        # Parse scenes into structured models
        scenes = []
        for scene_data in result.get("scenes", []):
            # Handle media type parsing
            media_type_str = scene_data.get("suggested_media_type", "image").lower()
            media_type = MediaType.IMAGE if media_type_str == "image" else MediaType.VIDEO
            
            scene = SceneStep(
                scene_number=scene_data["scene_number"],
                description=scene_data["description"],
                time_start=float(scene_data["time_start"]),
                time_end=float(scene_data["time_end"]),
                suggested_media_type=media_type,
                visual_prompt_draft=scene_data["visual_prompt_draft"],
                audio_context=scene_data.get("audio_context", ""),
                mood=scene_data.get("mood", "neutral"),
                suggested_transition=scene_data.get("suggested_transition"),
                voice_direction=self._parse_voice_direction(scene_data.get("voice"))
            )
            scenes.append(scene)
        
        # Sort by time_start to ensure correct order
        scenes.sort(key=lambda s: s.time_start)
        
        plan = MasterPlan(
            project_name=result.get("project_name", "Untitled Project"),
            total_duration=float(result.get("total_duration", duration)),
            scenes=scenes
        )
        
        # Validate coverage
        if not plan.validate_coverage():
            print("Warning: Scene plan has gaps or doesn't cover full duration")
        
        return plan
    
    async def refine_plan(
        self, 
        plan: MasterPlan, 
        feedback: str
    ) -> MasterPlan:
        """
        Refine an existing plan based on user feedback.
        
        Args:
            plan: The current plan to refine
            feedback: User feedback on what to change
        
        Returns:
            Refined MasterPlan
        """
        
        current_plan_json = plan.model_dump_json(indent=2)
        
        prompt = f"""
Here is the current video plan:
{current_plan_json}

User feedback to incorporate:
{feedback}

Please refine the plan based on this feedback. Maintain the same JSON structure.
Return the complete refined plan as JSON.
"""
        
        result = await self.llm.generate_json(
            prompt=prompt,
            system=PLANNER_SYSTEM_PROMPT,
            temperature=0.5
        )
        
        # Parse into MasterPlan (same logic as create_plan)
        scenes = []
        for scene_data in result.get("scenes", []):
            media_type_str = scene_data.get("suggested_media_type", "image").lower()
            media_type = MediaType.IMAGE if media_type_str == "image" else MediaType.VIDEO
            
            scene = SceneStep(
                scene_number=scene_data["scene_number"],
                description=scene_data["description"],
                time_start=float(scene_data["time_start"]),
                time_end=float(scene_data["time_end"]),
                suggested_media_type=media_type,
                visual_prompt_draft=scene_data["visual_prompt_draft"],
                audio_context=scene_data.get("audio_context", ""),
                mood=scene_data.get("mood", "neutral"),
                suggested_transition=scene_data.get("suggested_transition"),
                voice_direction=self._parse_voice_direction(scene_data.get("voice"))
            )
            scenes.append(scene)
        
        scenes.sort(key=lambda s: s.time_start)
        
        return MasterPlan(
            project_name=result.get("project_name", plan.project_name),
            total_duration=float(result.get("total_duration", plan.total_duration)),
            scenes=scenes
        )
    
    def _parse_voice_direction(self, voice_data: dict) -> Optional[VoiceDirection]:
        """Parse voice direction from LLM JSON response"""
        if not voice_data:
            return None
        
        try:
            # Parse voice type
            voice_type_str = voice_data.get("voice_type", "none").lower()
            voice_type_map = {
                "narrator": VoiceType.NARRATOR,
                "character": VoiceType.CHARACTER,
                "inner_thought": VoiceType.INNER_THOUGHT,
                "none": VoiceType.NONE
            }
            voice_type = voice_type_map.get(voice_type_str, VoiceType.NONE)
            
            # Parse gender
            gender_str = voice_data.get("gender", "androgynous").lower()
            gender_map = {
                "masculine": VoiceGender.MASCULINE,
                "feminine": VoiceGender.FEMININE,
                "androgynous": VoiceGender.ANDROGYNOUS
            }
            gender = gender_map.get(gender_str, VoiceGender.ANDROGYNOUS)
            
            # Helper to clamp values
            def clamp(val, min_v=0.0, max_v=1.0):
                try:
                    return max(min_v, min(max_v, float(val)))
                except (TypeError, ValueError):
                    return 0.5
            
            return VoiceDirection(
                voice_type=voice_type,
                should_speak=voice_data.get("should_speak", False),
                tone=clamp(voice_data.get("tone", 0.5)),
                cadence=clamp(voice_data.get("cadence", 0.5)),
                warmth=clamp(voice_data.get("warmth", 0.5)),
                solemnity=clamp(voice_data.get("solemnity", 0.5)),
                gender=gender,
                age_hint=voice_data.get("age_hint", "adult"),
                accent_hint=voice_data.get("accent_hint"),
                dialogue_text=voice_data.get("dialogue_text"),
                voice_notes=voice_data.get("voice_notes")
            )
        except Exception as e:
            print(f"Warning: Failed to parse voice direction: {e}")
            return None
